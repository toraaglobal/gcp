-------------------------------------------------------
Objectives
-------------------------------------------------------
    - Migrate existing Spark jobs to Cloud Dataproc
    - Modify Spark jobs to use Cloud Storage instead of HDFS
    - Optimize Spark jobs to run on Job specific clusters
Using
    Cloud Dataproc
    Apache cluster 


- Activate accout and list project 
    gcloud auth list
    gcloud config list project


- Check project permission within identity and access management
    ensure that the default compute Service Account {project-number}-compute@developer.gserviceaccount.com 
    is present and has the editor role assigned.

------------------------------------------------------
--- Migrate existing Spark jobs to Cloud Dataproc----
------------------------------------------------------

- Configure and start a Cloud Dataproc cluster
    In the GCP Console, on the Navigation menu, in the Big Data section, click Dataproc.
    Click Create Cluster.
    Enter sparktodp for Cluster Name.
    In the Versioning section, click Change and select 2.0 (Debian 10, Hadoop 3.2, Spark 3.1).


- Clone the source repository for the lab
    git -C ~ clone https://github.com/GoogleCloudPlatform/training-data-analyst


- To locate the default Cloud Storage bucket used by Cloud Dataproc enter the following command in Cloud Shell:
    export DP_STORAGE="gs://$(gcloud dataproc clusters describe sparktodp --region=us-central1 --format=json | jq -r '.config.configBucket')"


- To copy the sample notebooks into the Jupyter working folder enter the following command in Cloud Shell:
    gsutil -m cp ~/training-data-analyst/quests/sparktobq/*.ipynb $DP_STORAGE/notebooks/jupyter


- Log in to the Jupyter Notebook
    On the Dataproc Clusters page wait for the cluster to finish starting and then click the name of your cluster to open the Cluster details page.
    Click Web Interfaces.
    Click the Jupyter link to open a new Jupyter tab in your browser.
    This opens the Jupyter home page. Here you can see the contents of the /notebooks/jupyter directory in Cloud Storage that now includes the sample Jupyter notebooks used in this lab.
    Under the Files tab, click the GCS folder and then click 01_spark.ipynb notebook to open it.
    Click Cell and then Run All to run all of the cells in the notebook.
    Page back up to the top of the notebook and follow as the notebook completes runs each cell and outputs the results below them.



-------------------------------------------------------------
----    Separate Compute and Storage           --------------
-------------------------------------------------------------

- Modify Spark jobs to use Cloud Storage instead of HDFS
    Taking this original 'Lift & Shift' sample notebook you will now create a copy that decouples the storage 
    requirements for the job from the compute requirements. In this case, all you have to do is replace the 
    Hadoop file system calls with Cloud Storage calls by replacing hdfs:// storage references with gs:// references in 
    the code and adjusting folder names as necessary.

- In the Cloud Shell create a new storage bucket for your source data.
    export PROJECT_ID=$(gcloud info --format='value(config.project)')
    gsutil mb gs://$PROJECT_ID


- In the Cloud Shell copy the source data into the bucket.
    wget https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data_10_percent.gz
    gsutil cp kddcup.data_10_percent.gz gs://$PROJECT_ID/


- Switch back to the 01_spark Jupyter Notebook tab in your browser

- Click File and then select Make a Copy

- When the copy opens, click the 01_spark-Copy1 title and rename it to De-couple-storage

- Open the Jupyter tab for 01_spark

- Click File and then Save and checkpoint to save the notebook.

- Click File and then Close and Halt to shutdown the notebook.

- Switch back to the De-couple-storage Jupyter Notebook tab in your browser, if necessary.
    You no longer need the cells that download and copy the data onto the cluster's internal HDFS file system so you will remove those first.


- Delete the initial comment cells and the first three code cells ( In [1], In [2], and In [3]) so that the notebook now starts with the section Reading in Data.



---------------------------------------------------------------------
------- Deploy Spark Jobs               -----------------------------
---------------------------------------------------------------------

- Create a launch script.
    nano submit_onejob.sh

    #!/bin/bash
    gcloud dataproc jobs submit pyspark \
        --cluster sparktodp \
        --region us-central1 \
        spark_analysis.py \
        -- --bucket=$1


- Make the script executable:
    chmod +x submit_onejob.sh


- Launch the PySpark Analysis job:
    ./submit_onejob.sh $PROJECT_ID

    



